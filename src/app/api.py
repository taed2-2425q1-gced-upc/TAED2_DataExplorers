"""
Main script: it includes our API initialization and endpoints.
"""

import os
import pandas as pd
import numpy as np
import logging
import pickle
import tempfile
from contextlib import asynccontextmanager
from http import HTTPStatus
from typing import Dict
from pathlib import Path

from codecarbon import EmissionsTracker
from fastapi import FastAPI, UploadFile
from fastapi.responses import JSONResponse


from src.config import METRICS_DIR
from src.features import preprocessing
MODELS_FOLDER_PATH = Path("models")

# Initialize the dictionary to group models by "tabular" or "image" and then by model type
model_wrappers_dict: Dict[str, Dict[str, dict]] = {"image": {}}

'''
def file_to_image(file: bytes):
    """
    Reads an image file and formats it for the model.

    Parameters
    ----------
    file:
        bytes: The image file to classify.

    Returns
    -------
    Tensor: The image formatted for the model.
    """
    image = tf.io.decode_image(file, channels=3, dtype=tf.float32)
    return tf.image.resize(image, [224, 224])
'''

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Loads all pickled models found in `MODELS_DIR` and adds them to `models_list`"""

    with open(MODELS_FOLDER_PATH / "model.pkl", "rb") as pickled_model:
        cv_model = pickle.load(pickled_model)

    model_wrappers_dict["image"]["cnn"] = {
        "model": cv_model,
        "type": "cnn",
    }

    yield

    # Clear the list of models to avoid memory leaks
    # del model_wrappers_dict["tabular"]
    del model_wrappers_dict["image"]
   


# Define application
app = FastAPI(
    title="Landscape image classifier",
    description="This API lets you classify the Intel Image Classification dataset using a CNN model.",
    version="0.1",
    lifespan=lifespan,
)


@app.get("/", tags=["General"])  # path operation decorator
async def _index():
    """Root endpoint."""

    response = {
        "message": HTTPStatus.OK.phrase,
        "status-code": HTTPStatus.OK,
        "data": {"message": "Welcome to the landscape image classifier! Please, read the `/docs`!"},
    }
    return response


# Create and endpoint to classify an image
@app.post("/predict/image/", tags=["Prediction"])
async def _predict_image(file: UploadFile):
    """
    Classifies landscape images using a pre-trained CNN model.

    Parameters
    ----------
    file : UploadFile
        The image to classify.
    """
    # Read the image file and format it for the model
    image_stream = await file.read()

    with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp:
        tmp.write(image_stream)
        tmp_path = tmp.name  # Obtener la ruta del archivo temporal

    try:
        x_processed_image = preprocessing.process_images(tmp_path, [], [], 100, needs_return=True)
        x_processed_image = preprocessing.list_to_nparray(x_processed_image)
        cv_model = model_wrappers_dict["image"]["cnn"]["model"]
        with EmissionsTracker(
            project_name="image-classification",
            measure_power_secs=1,
            tracking_mode="process",
            output_dir=METRICS_DIR,
            output_file="emissions_api.csv",
            on_csv_write="append",
            default_cpu_power=45,
        ):
            predictions = cv_model.predict(x_processed_image)
        pd.read_csv(METRICS_DIR / "emissions_api.csv")
        predictions_dict = {preprocessing.getcode(i): predictions.tolist()[0][i] for i in range(6)}
    except Exception as e:
        return {"error": str(e)}
    await file.close()

    predicted_label = preprocessing.getcode(np.argmax(predictions))

    logging.info("Predicted class %s", predicted_label)

    response = {
        "Message": HTTPStatus.OK.phrase,
        "Status-code": HTTPStatus.OK,
        "Data": {
            "Model": "Convolutional Neural Network",
            "The prediction scores are": predictions_dict,
            "The predicted class is": predicted_label,
        },
    }

    return response

# New endpoint to return emissions generated by the prediction
@app.get("/emissions/", tags=["Emissions"])
async def _get_emissions():
    """
    Returns the emissions generated during the prediction.
    """
    emissions_file = os.path.join(METRICS_DIR, "emissions_api.csv")

    if not os.path.exists(emissions_file):
        return JSONResponse(
            status_code=HTTPStatus.NOT_FOUND,
            content={"message": "Emissions data not found."},
        )

    try:
        # Read the emissions file and return the latest record
        with open(emissions_file, "r") as f:
            lines = f.readlines()
            last_line = lines[-1]  # Get the last recorded emissions data
            emissions_data = last_line.strip().split(",")
            
            # Example structure (adjust depending on your csv format)
            emissions_response = {
                "Emissions in kg": float(emissions_data[5]),
                "Energy consumed in kWh": float(emissions_data[13]),
            }

        return emissions_response

    except Exception as e:
        return JSONResponse(
            status_code=HTTPStatus.INTERNAL_SERVER_ERROR,
            content={"message": f"Error reading emissions data: {str(e)}"},
        )